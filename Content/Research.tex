\chapter{Research and Architecture Definition}
\section{Maintainability}
Maintainability was declared a main goal in the requirements, and this is an issue with much broader reach than just good writing clean code.

Non-coding factors that contribute to maintainability of a software project include
\begin{itemize}
	\item ease of collaboration
	\item issue tracking
	\item early indication of breaking changes
	\item incremental integration of changes with possibility to roll back to an earlier point in history
\end{itemize}

The importance of such "meta-programming" techniques is testified by the fact that a job field for software engineers called \gls{DevOps} exists, which engages in finding ways to optimize coding workflow.
A definition of DevOps in \citet{Bass2015}:
\begin{quote}
	DevOps is a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality.
\end{quote}

Any time saved locating, fixing and testing mistakes translates to higher productivity of the development team. Although this field is too extensive to thoroughly investigate in the course of this thesis, some DevOps practices are outlined in the following sections and adapted to make the resulting software more easily maintainable.

In addition to the above, code itself should also conform to certain rules:
\begin{itemize}
	\item encapsulation of functionality in logical units - i.e. classes according to the implementation of \gls{oop} principles in C++
	\item common code formatting guidelines, i.e. making sure code from different contributors "looks the same"
	\item consistent code documentation
\end{itemize}

Adhering to above rules increases productivity of developers and lowers the barrier of entry for new coders.

\subsection{Version Control}
\gls{Version Control System}s  are used in \gls{scm} to keep track of changes to a codebase. They provide featues like
\begin{itemize}
	\item Bookkeeping of who authored which change
	\item Messages describing each change (obsoleting changelogs)
	\item History-keeping of earlier revisions of code
	\item Automatic conflict resolutions in case two developers changed the same files
	\item Maintaining separate states of the code in so-called branches
\end{itemize}

Their use is often mandatory in industry due to traceability requirements, but even single-developer project can benefit from using, a \gls{vcs}, e.g. because the branching feature coupled with changelog messages allows working on multiple separate features in parallel, merging changes back to the main program after a features has been completed. 

Most major \gls{vcs}es have ecosystems that allow for hosting open-source repositories on the \gls{cloud}\footnote{e.g. \url{https://sourceforge.net/create/}}, selection is mostly a matter of personal preference, though some differences exist.
An overview over some popular \gls{vcs}es follows that are available on both Windows and Linux.

\subsubsection{Apache Subversion}
Subversion\furl{https://subversion.apache.org/} is a centralized \gls{vcs}, changes made locally need to be pushed to a central server, where they are revisioned. It supports branching, tagging and commit messages.

Due to the centralization, no interaction with the \gls{vcs} is possible when the connection to the server is interrupted.

\subsubsection{Mercurial}
Mercurial\furl{https://www.mercurial-scm.org/} decentralized \gls{vcs}, where a checkout contains the whole repository, i.e. all interaction like committing changes, branching, checking out other code revisions can be done locally, while a central repository can be used to synchronize local changes with others.

\subsubsection{Git}
\gls{git}\furl{https://git-scm.com/} is like Mercurial, only popular\furl{https://www.openhub.net/repositories/compare}. It is also a distributed \gls{vcs}, but conceived by Linus Torvalds of Linux kernel fame on a search for ways to manage the large scale of contributors on the project.

Like with Mercurial, key benefits are
\begin{description}
	\item[Decentralization] No reliance on availability of a central server for daily work
	\item[Smart merging] Conflict resolution algorithms are fairly good at finding and automatically fixing trivial conflicts, keeping developer interaction on merges to a minimum
	\item[Central repository support] Though git operates locally, it supports pushing code to and pulling from remote repositories, enabling the use of a central "synchronization" repository, which is the Basis for the popular \gls{github} 
\end{description}

The distinguishing factor is the huge userbase that has resulted in git integration to a multitude of third-party tools centered around github\furl{http://github.com}, the open-source \gls{cloud}-repository provider for git.

\subsection{Continuous Integration}
\gls{ci} is a concept popularized in the rise of agile software development. In classical development structures, features are are coded to perceived completion before they are introduced into the codebase of a product, their functionality maybe tested in unit-tests. Integrations of features are performed on a fixed schedule, e.g. quarterly with integration tests following the procedure, leading to an eventual release.

\gls{ci} is a method to avoid the time- and effort-consuming "big-bang" integrations, by testing changes under development on the current state of the main codebase \textit{continuously}. This is made possible by \gls{vcs}es and their branching functionality, and works by taking the local branch of the developers code on each change, trying to build the code and running available tests on it. Breaking changes thus get reported to the developer much quicker than they would classically, limiting the possible causes of the build failure, thus reducing the time spent bug hunting.

Since \gls{ci} needs an always-on system in a consistent configuration that is always ready to build the code, this technique relies on virtualization through software like docker\furl{https://www.docker.com/} and VMWare\furl{https://www.vmware.com/}.

Since it is rarely feasible for small projects to maintain such build systems themselves, \gls{cloud} providers have started providing those as a service, for open-source projects often for free.

\subsubsection{Travis CI}
Travis \gls{ci}\furl{https://travis-ci.org} can run Linux (Ubuntu 12.04 and 14.04) and Mac OS X builds on their cloud platform.

It integrates with github for starting a build on each commit and reports completion status back to github.

Builds are queued on commit, depending on the load of servers it can take some time for them to actually execute.

Configuration happens in a .travis.yml file that has to be provided with the sources and the service is free for open source proojects.

\subsubsection{Circle CI}
Circle CI\furl{https://circleci.com} distinguishes itself from Travis through its larger selection of preconfigured docker containers, reducing configuration overhead of the build environment.
Configuration is through a .circleci file in the sources.

One Linux-based container is free, allowing one build at a time. MacOS X is paid only.

Like with Travis, they integrate with Github.

\subsubsection{Appveyor CI}
Appveyor\furl{https://ci.appveyor.com/login} is a Windows-only \gls{ci} provider, enabling builds on the various Microsoft Visual Studio version.

It provides github integration, the build is configured through an appveyor.yml file in the sources, and is free for open source projects.

\subsection{Code formatting guidelines}
Creating a common code format can be as simple as writing down a set of rules in a code-of-conduct letter for coders to adhere to. What constitutes a "good" formatting guideline is a highly subjective matter however, which is the reason for "different looking code" in the first place.

While it is unlikely that consensus will be reached on coding style in the full C++ community, companies and larger \gls{floss} projects often define guidelines for their employees / contributors,
e.g. the WebKit project\footnote{\url{https://webkit.org/code-style-guidelines/}} and Google\footnote{\url{https://github.com/google/styleguide}}

Though it is possible to manually review every contribution and deny it if it violates code syle guidelines, doing so is unproductively used developer time. Since reformatting code (in the case of C++) simply consists of parsing and reorganizing characters in a text file, it can be automated.

Tools like clang-format\footnote{\url{https://clang.llvm.org/docs/ClangFormat.html}} and astyle\footnote{\url{http://astyle.sourceforge.net/}} exist that - when supplied a formatting contract and source files, automatically reformat the latter.

When used in conjunction with a \gls{vcs}, formatting can be automatically applied before publishing changes (e.g. git pre-commit hook) to other parties, completely abstracting the developer from the process - and thus enabling him to freely disregard code style guidelines during development, without violating them on check-in.

\subsection{Unit Testing frameworks}

\todo{"nobody has the time to write tests"}

\subsection{Model-driven Design and Round-Trip Engineering}
Modeling is often the first step of implementing a fairly complex architecture, 
\todo{describe UML}
decomposing the goal into subobjectives and defining clear interfaces, enabling parallel execution when working in a team.

UML

Models - when, like documentation, manually created - are cumbersome to maintain and keep synchronized with necessary changes to the architecture discovered during implementation.

In classic Model-driven Design, the UML model can be used to generate boilerplate parts of the code (e.g. class / namespace declarations) to save the implementers some work.

Architectural changes made in the code have to be manually introduced in the model as well, which can not be enforced and often leads to the model desyncing. Because of that, often a waterfall model of project design is adopted, where the model is created as a starting point of implementation, and after it has been reviewed and implementation begins, discarded.

\gls{rte} Tooling helps alleviate this problem, by automatically feeding changes from the code back into the UML model.

Depending on the target language, the task of reintroducing changes can be very difficult. C++, being a very syntactically complex language is notoriously difficult to round-trip engineer, reflecting in bad tool support.o

In research, only a single open source / free-for-education modeling tool could be identified that offers \gls{rte} for C++: Astah\furl{http://astah.net}

Exporting a model to C++ code is provided as core functionality, import from code through a plugin\furl{http://astah.net/features/cpp-reverse-plugin}.

During development the reversing functionality was found to not work reliably, so \gls{rte} was not used.

\begin{quote}
Theory and practice sometimes clash. And when that happens, theory loses. Every single time.
- Torvalds, Linus (2009-03-25). Message to Linux kernel mailing list. Retrieved on 2009-03-25.
\end{quote}

\subsection{Autogenerated Docs}
\todo{docs}

\section{Extensibility}

\subsection{Plugin Architecture}
Looking at the requirements, it is apparent that almost all component have disjunct responsibilities, the main common element being the string description of the PFC.

The \gls{lsys} generator takes some input and outputs a model string. GUI, SVG and PDF renderings all operate on the model, but don't influence each others operation.

This characteristic is conducive to segregating those components of the app not just to different classes, but to completely separate compilation units, i.e. standalone libraries called plugins.

In conjunction with a dynamic loading system, this confers several benefits:

\begin{itemize}
	\item  Reduced compilation time of the main application
	\item  "Pluggability" of new functionality without recompilation of the main app
	\item  Lower startup times, as functionality can be loaded once it is needed, keeping the size of the the core binary small
\end{itemize}

Drawbacks to this implementation are
\begin{itemize}
	\item Updating the main app with changes breaking public APIs used by plugins will fail at runtime until all plugins have been updated/recompiled
	\item If plugins have dependencies on other plugins, maintenance complexity grows exponentially and loading sequence must be enforced
\end{itemize}

The additionally introduced complexity can - in sufficiently complex projects - significantly outweigh modularization benefits, resulting in what is called "plugin hell"\furl{https://queue.acm.org/detail.cfm?id=1053345}.
The Eclipse IDE, in its core being a plugin loader and all functionality supplied by plugins is a prime example of the possible complexity.

Since plugins to pfcrender will mainly consist of new Import/Export formats with limited intrinsic complexity and no dependencies on other plugins, a plugin architecture was deemed beneficial for increasing extensibility and thus the following architecture was chosen.

\todo{image of Plugin architecture}

\subsection{Configuration}
The solution is required to be scriptable, which means all configuration options necessary for automated operation must be selectable from the command line.
This focus on a feature-rich \gls{cli} leads to the necessity of good \gls{cli} documentation, i.e. a form of "help" message that documents the options available to the user.
Providing and maintaining a help page manually is not only cumbersome, it will, as is the case with all manually maintained documentation eventually become out-of-sync with the code.

This problem is exacerbated by use of a plugin architecture, as available options depend on the presence of plugins that use them. This in turn is runtime information that can not be known at compile time, thus making any of the standard, static methods like "hardcoded" help message, manpage or README files impossible to use.

Though it was not a requirement, it is desirable to provide the \gls{gui} with a method of setting configuration options as well, to eventually enable users that don't need scriptability to operate the application fully from the \gls{gui}

We thus need a method of configuration that is easily extensible with new options, accessible from \gls{cli} and \gls{gui} and generates \gls{cli} documentation automatically at runtime.

\subsubsection{Singleton Pattern}
\label{sec:ston}
Whenever a piece of information needs to be known in global scope, i.e. across the whole application and exists only once - as is the case for a store of configuration information - the Singleton, described in \citet{Gamma1994} is a very useful design pattern. 

\begin{lstlisting}
class Config_Registry {
private:
    static Config_Registry* instance; 
    Config_Registry();

public:
    static Config_Registry* getInstance()
    {
      if (instance == nullptr)
          instance = new Config_Registry();
      return instance;
    }
    vector<config_opt_T> conf_opts;
    ~Config_Registry();
    Config_Registry(Config_Registry&) = delete;
    Config_Registry operator=() = delete;
}

\end{lstlisting}
Key element of this class is the private constructor, which can only be executed through the class-static getInstance() method, which returns an already existing instance, or creates a new one.
Copying and moving are prevented by the delete copy and assignment operators - presence of which prevents autogeneration of move constructors by the compiler.
It follows that an object so-created can exist only once in the program and can be accessed from any file that can access the getInstance method, i.e. has the header included. 
This class can then contains a list of config options and other elements of interest to multiple parts of the program. 

Since the pointer has static duration, the lifetime of its object only ends when delete is called explicity, so care has to be taken to ensure its proper destruction and prevent memory leaks.

\subsection{Commandline Parsing}
Unlike other programming languages (e.g. Python), C++ does not provide a standard built-in facility to provide parsing capability for parsing parameters passed to the program as commandline options.

In a plugin-based architecture, the program must provide an \gls{cli} that is easily extended with additional commandline options, as plugins can need additional configuration and are loaded at runtime.

The main commandline parsing solution available in any standard C++ environment is getopt(), which is inherited from C and thus brings with it a procedural interface that is not easily extensible.

Alternatives:

\begin{itemize}
\item Manually building a structure to parse the argv array. While this is the most flexible solution w.r.t. customization, it is reinventing the wheel and also very inflexible with respect to extensibility.
\item Boost.progam\_options - A C++ native, highly configurable solution shipped with the open source Boost library\furl{http://www.boost.org/doc/libs/1\_58\_0/doc/html/program\_options.html}
\item QtCommandLineParser - A Qt native parsing class. Batch-parses all parameters and returns data structures with positional arguments in correct order, and a (reordered) map of switches with their parameters
http://doc.qt.io/qt-5/qcommandlineparser.html
\end{itemize}



\section{Option Persistency}
Since it is often unneccessary to set any and all config options for the program on the command line each time it is run, keeping config options persistently over program relaunches is useful. The typical implementation of this persistent store again varies by platform, ranging from putting keys into the Windows registry, over property list files on Mac to plain .ini or .cfg files on Linux.

Again, Qt comes with a wrapper around this platform disparity



\section{Cross-OS operation}

Writing portable software has historically been difficult, as operating system \gls{api}s differ significantly. 

Linux and MacOS comply with the \gls{posix} standard, which defines an \gls{api} for system calls, while Windows has historically had a non-\gls{posix}-compliant proprietary system programming \gls{api}.

Recently, Microsoft is making advances towards \gls{posix} compatibility in Win10\furl{https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/}, and the Linux world offers a similar compatiblility layer with WINE\furl{https://en.wikipedia.org/w/index.php?title=Wine_(software)&oldid=823689556}. While the performance of the windows POSIX layer is promising\furl{https://www.phoronix.com/scan.php?page=article&item=windows-10-lxcore&num=2}, neither it, nor WINE\furl{https://wiki.winehq.org/Performance} are on par with programs implementing each systems native interface directly at the time of writing.

Supporting the OS native APIs is thus necessary, since good application performance is a requirement.

Notable \gls{api} differences between \gls{posix} and Windows are
\begin{description}
	\item[Library Loading] \gls{posix} Systems use dlopen(), Windows uses LoadLibrary() and different runtime library layouts (.dll on Windows, .so on \gls{posix}
	\item[3D Graphics Backend] Windows mainly uses Microsoft GDI or DirectX and - more recently - AMD Vulkan, while the \gls{posix} world mostly uses OpenGL.
\end{description}

Code depending on any of the above directly is therefore nonportable and the multitude of incompatibilities makes implementing a cross-platform program from-scratch, i.e. without depending on OS-abstracting 3rd-party libraries - an arduous task.

Thankfully, several of those libraries exist that provide a higher-level API abstracting programmers from OS-specific code. They differ wildly in feature-scope, some being abstractions of the graphics layer, some being full-featured cross-platform \gls{gui} toolkits and will be investigated in the following section.

\subsection{Graphics Frameworks Providing OS Abstraction}
\label{sec:res_frameworks}

\subsubsection{Cairo}
\label{sec:cairo}
Cairo is a C-based 2D vector graphics library which is widely used in open-source projects\furl{https://en.wikipedia.org/w/index.php?title=Cairo_(graphics)&oldid=812908471} like GTK+, WebKit and the Poppler PDF rendering library.

\subsubsection{SFML}
The C++ written \gls{sfml} library\furl{https://en.wikipedia.org/w/index.php?title=Simple_and_Fast_Multimedia_Library&oldid=820377932} is a relative newcomer to the multi-OS application landscape, with v1 released in 2007.

While its core is a 2D graphics engine, add-ons exist for providing GUI application functionality.  

At the time of writing, it is mainly used in in free or indie game releases.

\subsubsection{SDL}
The \gls{sdl} is a cross-platfrom \gls{hal} library created in 1998, with extensive use in games industry\furl{https://en.wikipedia.org/w/index.php?title=List_of_games_using_SDL&oldid=819037514} to provide 2D \& 3D rendering.

\subsubsection{GTK+}
Unlike above solutions, GTK+\furl{https://en.wikipedia.org/w/index.php?title=GTK\%2B&oldid=822745199}, written in C - is a full GUI application toolkit, providing not only a \gls{hal}, but also graphical control elements, called widgets. Since 2005 it uses cairo (\ref{sec:cairo}) to provide its low-level rendering functionality.

\subsubsection{Qt Framework}
The Qt Framework, like GTK+ is a full-featured GUI application development toolkit and offers extensive functionality.

Like GTK+ it has a widget-based API, but since Qt5 additionally brings a JavaFX-like declarative API called QtQuick.

In addition to providing graphics and \gls{gui} abstractions, it also provides a platform-independent library loading wrapper.

Asynchronous GUI programming is provided through an event loop and notification system called Signals and Slots

\subsection{Makefile generators}
\label{sec:resMakefileGen}
Having cross-platform compilable sources is not the only requirement to getting a program developable and deployable cross-platform.
In addition to sources and headers provided with the project, compiling needs information on 
\begin{itemize}
	\item which compiler/linker toolchain is available and where
	\item where all the libraries the project links to are located
	\item if any platform specific preprocessor directives or linker flags have to be set
	\item Build type (Debug/Release) and architecture ( arm, x86, amd64 etc.)
\end{itemize}

This information is typically stored in "project stores" like Visual Studio's solution, or makefiles. Since those are inherently platform-specific, and manually creating one for each project type and platform is cumbersome at best, so called makefile generators were created, which parse build information from script files written by the programmer and then identify actual locations of dependencies and availabe tools automatically. If the process is successful and all dependencies are found, they output a build definition (makefile, Visual Studio solution or similar) that runs the compiler with all necessary options. Often those tools can also create automated deployments for the software they are building, e.g. generate a make install target. The most widely used ones are discussed here

\subsubsection{GNU autotools}
A collection of the GNU utilities autoconf, automake and libtool. It is the first widely used cross platform build configurator, existing since the 1980s. They are used to generate a ./configure script, that can be executed on any system that provides a bourne-shell (standard on POSIX systems) and generates a makefile.

Cross-compilation to Windows from a linux host is possible using MinGW. 
\todo{asdf}
\subsubsection{QMake}
The native makefile generator of Qt. Defines build definitions in .pro files and handles Qt specific tools like the \gls{moc} automatically as needed, giving a rather straightforward experience when building Qt programs.

Integrated with QtCreator.

\subsection{CMake}
Gathers information on what to build from CMakeLists.txt files added to the project's sources.

Can generate projects in an plethora of formats, including Visual Studio, GNU make,  Apple Xcode and Eclipse

Has widespread usage in open source and commercial projects and QtCreator provides good integration for CMake generated projects.


